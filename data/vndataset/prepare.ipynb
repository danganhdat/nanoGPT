{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccaa0f-2d0a-40b7-b502-55e85aacc712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b622729-f740-433e-a430-c48be97ea7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5c19f0-5036-4b55-b941-b760842960e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/danganhdat3004/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b342964-0c99-41ca-b218-5843bb92c485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5078071e868f474e907fa6ab386849d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val_5M_tokens.bin:   0%|          | 0.00/9.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downloaded_val_path = hf_hub_download(repo_id=\"danganhdat/bins\", filename=\"val_5M_tokens.bin\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95afd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_train_path = hf_hub_download(repo_id=\"danganhdat/bins\", filename=\"train_10B_tokens.bin.gz\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095f4b4d-66de-43bf-9271-30230fbab0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_path = \"./val_5M_tokens.bin\"\n",
    "\n",
    "with open(downloaded_val_path, \"rb\") as f_src:\n",
    "    with open(val_path, \"wb\") as f_dst:\n",
    "        shutil.copyfileobj(f_src, f_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87ddcd6-5e41-4106-9a43-1ce295960b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"./train_10B_tokens.bin\"\n",
    "\n",
    "# Read the compressed file\n",
    "with gzip.open(compressed_train_path, \"rb\") as f_in:\n",
    "    with open(train_path, \"wb\") as f_out:\n",
    "        f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c63d60-0e06-47af-ac8b-9018a0c809bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = np.memmap(val_path, dtype=np.uint16, mode='r')\n",
    "print(f\"Length: {len(v):_}\")\n",
    "print(v[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13931801-3007-4a6e-9265-e7a6a4a5ce8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 9_903_287_246\n",
      "[ 1326  1734    25  3140  3352   703  2620  1798  2133   711  5630  2176\n",
      "  5606   649   484   477 12277    83   198   198     7    50    38 15448\n",
      "     8    13    12   902   456 16524    15    12    19  1192 14419  1980\n",
      "  6766    11 11891  2508  2693  3390  8183   922   898  1029  6909  1156\n",
      "   822    82  1760  1326  1734     8   801  1580    11  1932  2032  2389\n",
      "   581  1702  1195  1340  5630  2176  5606   649  5139   797  5430  1224\n",
      "  2139   561   870  1340   931   979    86  3287    83   484   477 12277\n",
      "    83  1796 11237  3332  3811  3417  3157  5606   649  1473  2634  6758\n",
      "    12    42   415 10434  1709 16525    15  1411  1447    11   581  3390\n",
      "  1796  2334   895   832  1427  4010   484   477 12277    83 16526 10895\n",
      "  1096  8508    13   198   198   499   493  1192 14419  1980  6766    11\n",
      "  1320  1148  1195  4010   870  1194   446  3496   801  7650   446   545\n",
      "    11   545   719  4295  1516  1703  2303 11329  2534   478  5612  3255\n",
      " 14185  4185  1745  2008   446  7718  1481   877  1744  2303   833  1165\n",
      "    13   478   480  2521  8967  3157  3502   797  4295  1516  1703  1481\n",
      "  1388  1528    11  1187  4346  6641   859  1295   934  1077  1099  3157\n",
      "   873  1669   833  1165    11  5656  2521  6664  3430  3157  2672  1062\n",
      "  1516  1703  2845   648    13   198 15618   561  1290   664   626  4010\n",
      "    11  6909  1156   822    82  7076  1427 16525    15    15  5908   648\n",
      "    11  2101   561   877   711  1473   801 16525   641   732  1473  1675\n",
      "  2370   478  5612  1760 10674   478  5612     8    11  5656]\n"
     ]
    }
   ],
   "source": [
    "t = np.memmap(train_path, dtype=np.uint16, mode='r')\n",
    "print(f\"Length: {len(t):_}\")\n",
    "print(t[:250])\n",
    "# 9_903_287_246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7f2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dfbe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ba9abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb37d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1041ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23ac4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
       "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
